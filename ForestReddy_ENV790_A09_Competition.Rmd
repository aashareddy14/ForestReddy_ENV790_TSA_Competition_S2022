---
title: "Project Report"
author: "Aasha Reddy"
date: "3/20/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r library, warning=FALSE, message=FALSE}
# load packages
library(lubridate)
library(ggplot2)
library(forecast)
library(Kendall)
library(tseries)
library(outliers)
library(tidyverse)
library(cowplot)
library(readxl)
```

# Introduction

In this assignment, we participate in a Kaggle competition 

# Data Cleaning


```{r}
# load data
demand <- read.csv("data/load.csv") %>%
  janitor::clean_names() 

humidity <- read.csv("data/relative_humidity.csv", skipNul = T) %>%
  janitor::clean_names()

temp <- read.csv("data/temperature.csv") %>%
  janitor::clean_names()
```

First we examine the demand dataset. We note that all rows after 2191 are NA, so we remove those rows. We also note that all meter_id's are 1, so we can remove that column from the dataset. We then average across all the hour columns to transform hourly data into total daily demand. We also transform the date variable into a datetime object using lubridate. In the final dataset, there are no NA values. In examining the humidity and temperature datasets, we also note that there is no missing data. We transform this data from hourly to daily as well to match the demand dataset. 

```{r}
# examine demand dataset
summary(demand)

# Demand dataset - transform hourly data into daily data
demand <- demand[1:2191, ]

demand <- demand %>%
  select(-meter_id) %>%
  mutate(date = mdy(date)) %>%
  pivot_longer(., cols = 2:25) %>%
  group_by(date) %>%
  summarize(demand = mean(value, na.rm = T))
```

```{r}
# examine humidity dataset
summary(humidity)

# clean humidity dataset - transform from houly to daily
humidity <- humidity %>%
  mutate(date = dmy(date)) %>%
  group_by(date) %>%
  summarize(
    rh_ws1 = mean(rh_ws1),
    rh_ws2 = mean(rh_ws2),
    rh_ws3 = mean(rh_ws3),
    rh_ws4 = mean(rh_ws4),
    rh_ws5 = mean(rh_ws5),
    rh_ws6 = mean(rh_ws6),
    rh_ws7 = mean(rh_ws7),
    rh_ws8 = mean(rh_ws8),
    rh_ws9 = mean(rh_ws9),
    rh_ws10 = mean(rh_ws10),
    rh_ws11 = mean(rh_ws11),
    rh_ws12 = mean(rh_ws12),
    rh_ws13 = mean(rh_ws13),
    rh_ws14 = mean(rh_ws14),
    rh_ws15 = mean(rh_ws15),
    rh_ws16 = mean(rh_ws16),
    rh_ws17 = mean(rh_ws17),
    rh_ws18 = mean(rh_ws18),
    rh_ws19 = mean(rh_ws19),
    rh_ws20 = mean(rh_ws20),
    rh_ws21 = mean(rh_ws21),
    rh_ws22 = mean(rh_ws22),
    rh_ws23 = mean(rh_ws23),
    rh_ws24 = mean(rh_ws24),
    rh_ws25 = mean(rh_ws25),
    rh_ws26 = mean(rh_ws26),
    rh_ws27 = mean(rh_ws27),
    rh_ws28 = mean(rh_ws28)
  )

average_humidity <- humidity %>%
  pivot_longer(2:ncol(humidity)) %>%
  select(-name) %>%
  group_by(date) %>%
  summarize(avg_humidity = mean(value, na.rm = T))

```


```{r}
# examine temperature dataset
summary(temp)

# clean temperature dataset - transform from hourly to daily
temp <- temp %>%
  mutate(date = dmy(date)) %>%
  group_by(date) %>%
  summarize(
    t_ws1 = mean(t_ws1),
    t_ws2 = mean(t_ws2),
    t_ws3 = mean(t_ws3),
    t_ws4 = mean(t_ws4),
    t_ws5 = mean(t_ws5),
    t_ws6 = mean(t_ws6),
    t_ws7 = mean(t_ws7),
    t_ws8 = mean(t_ws8),
    t_ws9 = mean(t_ws9),
    t_ws10 = mean(t_ws10),
    t_ws11 = mean(t_ws11),
    t_ws12 = mean(t_ws12),
    t_ws13 = mean(t_ws13),
    t_ws14 = mean(t_ws14),
    t_ws15 = mean(t_ws15),
    t_ws16 = mean(t_ws16),
    t_ws17 = mean(t_ws17),
    t_ws18 = mean(t_ws18),
    t_ws19 = mean(t_ws19),
    t_ws20 = mean(t_ws20),
    t_ws21 = mean(t_ws21),
    t_ws22 = mean(t_ws22),
    t_ws23 = mean(t_ws23),
    t_ws24 = mean(t_ws24),
    t_ws25 = mean(t_ws25),
    t_ws26 = mean(t_ws26),
    t_ws27 = mean(t_ws27),
    t_ws28 = mean(t_ws28)
  )

average_temp <- temp %>%
  pivot_longer(2:ncol(temp)) %>%
  select(-name) %>%
  group_by(date) %>%
  summarize(avg_temp = mean(value, na.rm = T))

average_temp_vector <- as.vector(average_temp$avg_temp)
```

```{r}
# merge all three data sets
df <- left_join(demand, average_temp, by = "date") %>%
  left_join(., average_humidity, by = "date")
```

# Exploratory Data Analysis 

First we will start by examining the demand data. We see that there is definitely a seasonal trend, and it looks like an increasing trend as well. We also see that the ACF plot has maybe a small scalloping pattern. 

```{r}
# plot time series of demand data
ggplot(demand, aes(x = date, y = demand)) +
  geom_line() +
  labs(
    title = "Time Series of Demand (2005 - 2010)",
    x = "Year",
    y = "Demand"
  )

# ACF and PACF plot
par(mar=c(3,3,3,0));par(mfrow = c(1, 2)) # adding so the plot titles are fully visible.
Acf(demand$demand, lag = 40, plot = T)
Pacf(demand$demand, lag = 40, plot = T)
```



# Methods and Modeling

In order to assess our models, we will choose the holdout period to be the final year in the dataset, 2010. We will thus train our models on data from 2005 - 2009, and then evaluate how well our models perform on the 2010 data, choosing the final model to be the one that acheives the highest MAPE. Finally, we will forecast demand for 2011 using our best model. 

```{r}
# split into test and training set
demand_train <- demand %>%
  filter(date >= "2005-01-01", date < "2010-01-01")

demand_test <- demand %>%
  filter(date >= "2010-01-01")

temp_train <- df %>%
  filter(date >= "2005-01-01", date < "2010-01-01") %>% 
  select(avg_temp) 
temp_train <- as.vector(temp_train$avg_temp)
 
test_temp <- df %>%
  filter(date >= "2010-01-01", date <= "2010-01-31") %>% 
  select(avg_temp) 
test_temp <- as.vector(test_temp$avg_temp)

humid_train <- df %>%
  filter(date >= "2005-01-01", date < "2010-01-01") %>% 
  select(avg_humidity) 
humid_train <- as.vector(humid_train$avg_humidity)
 
test_humid <- df %>%
  filter(date >= "2010-01-01", date <= "2010-01-31") %>% 
  select(avg_humidity) 
test_humid <- as.vector(test_humid$avg_humidity)

# create matrix of for xreg input
xreg_train <- cbind(temp_train, humid_train)
xreg_test <- cbind(test_temp, test_humid)
xreg_full <- cbind(df$avg_temp,df$avg_humidity)

```

We also transform the demand dataframe into a time series object here. 

```{r}
# transform demand into ts object for both training and test set
demand_train_ts <- ts(demand_train$demand, start = c(2005, 1, 1), frequency = 365)
#demand_train_ts <- msts(demand_train$demand, seasonal.periods = c(7, 365))
demand_test_ts <- ts(demand_test$demand, start = c(2010,1, 1), frequency = 365)
demand_ts <- ts(demand$demand, start = c(2005, 1, 1), frequency = 365)

# check 
head(demand_train_ts,15) 
plot(demand_train_ts)
```


## Model 1 - Seasonal Naive Model

The first model we try is a seasonal naive model. To do this we first decompose and deseason the demand series. 

```{r Decompose}
# Using R decompose function
decompose_demand <- decompose(demand_train_ts, "additive")
plot(decompose_demand)

# The ACF plot show a slow decay which is a sign of non-stationarity.
# Creating non-seasonal residential price time series because some models can't handle seasonality
deseasonal_demand <- seasadj(decompose_demand)

```

Next we fit the model using the training data, from 2005 - 2009. 

```{r Model 1 - SNAIVE fit}
# fit seasonal naive model - I'm not sure what h should be? I think 365 because we are working
SNAIVE_seas_2009 <- snaive(demand_train_ts, h = 31)
checkresiduals(SNAIVE_seas_2009)
```

We see that the residuals are normally distributed. However, there seems to be a pattern and significant correlation in the ACF. We hope to improve on this in our next model.

We then forecast daily demand for January 2010, seen in the plot below. The training MAPE is 23.05 while the test MAPE (on January 2010) is 42.36. The MAPE is a lot higher for the test data, meaning our seasonal naive model is overfitting to the training data.

```{r Model 1 - SNAIVE forecast test}
# forecast for Jan 2010

# test data - Jan 2021
test <- demand_test$demand[1:31]

par(mfrow = c(1, 1))
SNAIVE_scores_2009 <- accuracy(SNAIVE_seas_2009, test)
SNAIVE_scores_2009
plot(SNAIVE_seas_2009)

SNAIVE_forecast_Jan_2010 <- as.data.frame(SNAIVE_seas_2009$model$future)[1:31,] 
```

Now we refit the seasonal naive model using all data from 2005 - 2010, and then use that to forecast for January 2011. 

```{r Model 1 - SNAIVE forecast}
# now refit model using all data (2005 - 2010) - use demand_ts
SNAIVE_seas_2010 <- snaive(demand_ts, h = 31)

# load submission template and extract date column
submission_template <- read_excel("data/submission_template.csv", 
    sheet = "p90") %>% 
  select(date)

# forecast for Jan 2011 (this is what we submit to kaggle)
SNAIVE_forecast_Jan_2011 <- as.data.frame(SNAIVE_seas_2010$model$future) 
SNAIVE_forecast_Jan_2011 <- cbind(submission_template, SNAIVE_forecast_Jan_2011) %>%
  rename(load = x) %>%
  filter(date <= "2011-01-31")

# plot forecast
plot(SNAIVE_seas_2010)

# write csv
write.csv(SNAIVE_forecast_Jan_2011, "kaggle_output/SNAIVE_forecast.csv")
```

## Model 2 - Seasonal ARIMA model (Autofit)

Our second model will be a seasonal ARIMA model.  

First we fit the model using the training data, from 2005 - 2009, using the auto arima function. The auto arima chooses an ARIMA(2, 0, 2) model.

```{r Model 2 - SARIMA_auto fit}
# fit sarima autofit
SARIMA_autofit_2009 <- auto.arima(demand_train_ts)
checkresiduals(SARIMA_autofit_2009)
```

We see that the residuals here are normally distributed but do not seem to be that random. ACF shows significant correlations as well - this means that this likely will not be our best model!

Still, we move forward and forecast daily demand for January 2010, seen in the plot below. The MAPE on the test set (Januray 2010) is 25.1363 here, and we note that this is a large improvement from the seasonal naive model. We also see the plot forecast for Jan 2010 below. 

```{r Model 2 - SARIMA_auto test forecast}
# forecast for Jan 2010
SARIMA_forecast_Jan_2010 <- forecast::forecast(SARIMA_autofit_2009,h = 31)
SARIMA_scores_Jan_2010 <- accuracy(SARIMA_forecast_Jan_2010$mean,test)
SARIMA_scores_Jan_2010
plot(SARIMA_forecast_Jan_2010)
```

Now we refit the seasonal naive model using all data from 2005 - 2010, and then use that to forecast for January 2011. 

```{r Model 2 - SARIMA_auto forecast}
# now refit model using all data (2005 - 2010) - use demand_ts
SARIMA_autofit_2010 <- auto.arima(demand_ts)

# load submission template and extract date column
submission_template <- read_excel("data/submission_template.csv", 
    sheet = "p90") %>% 
  select(date) %>%
  filter(date <= "2011-01-31")

# forecast for Jan 2011 (this is what we submit to kaggle)
SARIMA_forecast_Jan_2011 <- forecast::forecast(SARIMA_autofit_2010,h=31)

# plot forecast
plot(SARIMA_forecast_Jan_2011)

SARIMA_forecast_Jan_2011 <- cbind(submission_template, 
                                  load = SARIMA_forecast_Jan_2011$mean)

# write csv
write.csv(SARIMA_forecast_Jan_2011, "kaggle_output/SARIMA_autofit_forecast.csv")
```

At this point, we submit both models to kaggle and ______ outperformed ______ on the unseen test set. We will continue trying to improve our models. 

# Model 3

Next we will fit a use auto.arima to fit a seasonal arima model that includes temperature as an exogenous variable. 

```{r}
SARIMAX_autofit_2009_temp <- auto.arima(demand_train_ts, xreg = temp_train)
checkresiduals(SARIMAX_autofit_2009_temp)
```
We see that the residuals here are normally distributed. However, the ACF shows significant correlations, similar to the original autofit SARIMA. 

Regardless, we will forecast demand for January 2010. The MAPE on the test set (Januray 2010) is 16.26, which is notably smaller that that of the SARIMA(1,2,2) model without temperature as an exogenous variable. The forecast is plotted below. 

```{r}
# forecast for Jan 2010
SARIMAX_forecast_test_temp <- forecast::forecast(SARIMAX_autofit_2009_temp, h = 31, 
                                                    xreg = test_temp)
SARIMAX_scores_test_temp <- accuracy(SARIMAX_forecast_test_temp$mean, test)
SARIMAX_scores_test_temp
plot(SARIMAX_forecast_test_temp)
```
Now we refit the seasonal naive model using all data from 2005 - 2010, and then use that to forecast for January 2011.  The January 2011 temperature is assumed to be the same as January 2010 temperature.

```{r}
# now refit model using all data (2005 - 2010) - use demand_ts
SARIMAX_temp_autofit_2010 <- auto.arima(demand_ts, xreg = average_temp_vector)

# load submission template and extract date column
submission_template <- read_excel("data/submission_template.csv", 
    sheet = "p90") %>% 
  select(date) %>%
  filter(date <= "2011-01-31")

# forecast for Jan 2011 (this is what we submit to kaggle)
SARIMAX_forecast_Jan_2011_temp <- forecast::forecast(SARIMAX_temp_autofit_2010,h = 31, xreg = test_temp) # ASSUMES THE SAME TEMP AS 2010! 

SARIMAX_forecast_Jan_2011_temp <- cbind(submission_template, 
                                  load = SARIMAX_forecast_Jan_2011_temp$mean)

# write csv
write.csv(SARIMA_forecast_Jan_2011, "kaggle_output/SARIMAX_temp_autofit_forecast.csv")
```



# Model 4
ARIMA + FOURIER 

Next we will use a autofit arima with seasonal fourier terms to account for multiple seasonalilty present in the data. The multiple seasonality comes from the the repeated weekly variation and seasonal monthly variation throughout the year. 

```{r msts creation}
# create multiple seasonal time series objects 
# training data set
demand_train_msts <- msts(demand_train$demand, 
                           seasonal.periods = c(7,365.25),
                           start = c(2005,1,1))
#full dataset
demand_msts <- msts(demand$demand, 
                           seasonal.periods = c(7,365.25),
                           start = c(2005,1,1))
```


```{r Model 4 train}
# ARIMA + Fourier Fit training data
ARIMA_Four_fit_2009 <- auto.arima(demand_train_msts, 
                             seasonal = FALSE, 
                             lambda = 0,
                             xreg = fourier(demand_train_msts, 
                                          K = c(2,4))
                             )
checkresiduals(ARIMA_Four_fit_2009)

#ARIMA + Fourier Test Fit for January 2010
ARIMA_Four_for_2010 <- forecast(ARIMA_Four_fit_2009,
                           xreg = fourier(demand_train_msts, 
                                        K = c(2,4),
                                        h = 31),
                           h = 31
                           ) 
ARIMA_Four_for_2010_scores <- accuracy(ARIMA_Four_for_2010$mean,test)
autoplot(ARIMA_Four_for_2010)
```

Next, we refit the model using all demand data. 
```{r Model 4 forecast}
# ARIMA + Fourier fit full data set 2005-2010
ARIMA_Four_fit_2010 <- auto.arima(demand_msts, 
                             seasonal = FALSE, 
                             lambda = 0,
                             xreg = fourier(demand_msts, 
                                          K = c(2,4))
                             )
# forecast Jan 2011
ARIMA_Four_for_2011 <- forecast(ARIMA_Four_fit_2011,
                           xreg = fourier(demand_msts, 
                                        K = c(2,4),
                                        h = 31),
                           h = 31
                           ) 

autoplot(ARIMA_Four_for_2011)


ARIMA_Four_for_2011 <- cbind(submission_template, 
                                  load = ARIMA_Four_for_2011$mean)

# write csv
write.csv(ARIMA_Four_for_2011, "kaggle_output/ARIMA_Four_for_2011.csv")
```


# Model 5
This model will implement a seasonal ARIMA model with temperature and humidity as exogenous regressors. Model order will be identified using the auto.arima function. 


```{r Model 5 train}
model5_train <- auto.arima(demand_train_ts, xreg = xreg_train, seasonal = TRUE)
checkresiduals(model5_train)
```

```{r Model 5 forecast, warning=FALSE}
# test forecast  for Jan 2010
model5_Jan2010_for <- forecast::forecast(model5_train, h = 31, 
                                                    xreg = xreg_test)
model5_scores_test <- accuracy(model5_Jan2010_for$mean, test)
model5_scores_test
plot(model5_Jan2010_for)

# now refit model using all data (2005 - 2010) - use demand_ts
model5 <- auto.arima(demand_ts, xreg = xreg_full)

# forecast for Jan 2011 (this is what we submit to kaggle)
Model5_Jan_forecast_Jan_2011_temp <- forecast::forecast(model5, h = 31, xreg = xreg_test) 
# ASSUMES THE SAME TEMP and HUMIDITY AS 2010! We could average January temp and humidity across years or forecast these values with another model. 

# plot forecast
plot(Model5_Jan_forecast_Jan_2011_temp)

Model5_Jan_forecast_Jan_2011_temp <- cbind(submission_template, 
                                  load = Model5_Jan_forecast_Jan_2011_temp$mean)

# write csv
write.csv(Model5_Jan_forecast_Jan_2011_temp, "kaggle_output/SARIMAX_temp_autofit_forecast.csv")

```


# Model 6
Some ideas for next models: 
- TBATS
- Neural Network Time Series Forecasts
- STL + ETS


# Results
```{r}
#create data frame
scores <- as.data.frame(
  rbind( SARIMA_scores_Jan_2010, SARIMAX_scores_test_temp, ARIMA_Four_for_2010_scores, model5_scores_test)
  )
row.names(scores) <- c( "SARIMA(1,2,2)","SARIMAX(3,0,2)_temp", "ARIMA + Fourier", "SARIMA + temp + humidity")

#choose model with lowest RMSE
best_model_index <- which.min(scores[,"RMSE"])
cat("The best model by RMSE is:", row.names(scores[best_model_index,]))                       
```

